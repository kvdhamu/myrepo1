
fetch_driver_logs = dsl.ContainerOp(
    name=spark_logs_job_name,
    image="{{ image }}",
    command=["sh", "-c"],
    arguments=[
        """
        set -e
        APP_NAME="{{ stage }}"  # SparkApplication name
        NAMESPACE="{{ project_namespace }}"
        DRIVER_POD="{{ submit_spark_job.outputs['driver_pod_name'] }}"
        
        echo "Waiting for SparkApplication $APP_NAME to start running..."
        
        # Monitor Spark job state in the background
        (
            while true; do
                STATUS=$(kubectl get sparkapplication $APP_NAME -n $NAMESPACE -o jsonpath='{.status.applicationState.state}')
                echo "$(date) - SparkApplication status: $STATUS"
                
                if [ "$STATUS" == "COMPLETED" ]; then
                    echo "Spark job completed successfully."
                    exit 0
                elif [ "$STATUS" == "FAILED" ] || [ "$STATUS" == "SUBMISSION_FAILED" ]; then
                    echo "Spark job failed!"
                    exit 1
                fi
                
                sleep 10
            done
        ) &

        # Stream logs with timeout mechanism
        echo "Streaming logs for driver pod: $DRIVER_POD"
        LAST_LOG_TIME=$(date +%s)

        while true; do
            kubectl logs $DRIVER_POD -n $NAMESPACE > /tmp/driver_pod.log
            NEW_LOG_TIME=$(date +%s)
            
            # Check if log file has new content
            if [ "$(stat -c %Y /tmp/driver_pod.log)" -gt "$LAST_LOG_TIME" ]; then
                cat /tmp/driver_pod.log
                LAST_LOG_TIME=$NEW_LOG_TIME
            fi
            
            # Stop if no new logs appear for 30s
            if [ $((NEW_LOG_TIME - LAST_LOG_TIME)) -gt 30 ]; then
                echo "No new logs for 30 seconds. Exiting log streaming."
                exit 0
            fi
            
            sleep 5
        done
        """
    ],
    file_outputs={
        "driver_logs": "/tmp/driver_pod.log"
    }
)
