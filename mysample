
from kfp import dsl
from kfp import kubernetes

@dsl.pipeline(
    name="Spark Job Pipeline",
    description="A pipeline to run a Spark job and export driver logs."
)
def spark_job_pipeline():
    # Define the SparkApplication YAML
    spark_app_yaml = """
    apiVersion: "sparkoperator.k8s.io/v1beta2"
    kind: SparkApplication
    metadata:
      name: spark-job
      namespace: kubeflow
    spec:
      type: Python
      pythonVersion: "3"
      mode: cluster
      image: "gcr.io/spark-operator/spark:v3.1.1"
      imagePullPolicy: Always
      mainApplicationFile: "local:///opt/spark/examples/src/main/python/pi.py"
      sparkVersion: "3.1.1"
      restartPolicy:
        type: OnFailure
      driver:
        cores: 1
        coreLimit: "1200m"
        memory: "512m"
        labels:
          version: 3.1.1
        serviceAccount: spark
      executor:
        cores: 1
        instances: 1
        memory: "512m"
        labels:
          version: 3.1.1
    """

    # Create the SparkApplication resource
    spark_op = kubernetes.ResourceOp(
        name="create-spark-job",
        k8s_resource=spark_app_yaml,
        action="create",
        attribute_outputs={
            "driver_pod_name": "{.status.driverInfo.podName}"
        }
    )

    # Fetch driver logs
    fetch_driver_logs = dsl.ContainerOp(
        name="fetch-driver-logs",
        image="bitnami/kubectl:latest",  # Use a kubectl image
        command=["sh", "-c"],
        arguments=[
            f"kubectl logs {spark_op.outputs['driver_pod_name']} -n kubeflow > /tmp/driver.log && "
            "cat /tmp/driver.log"
        ],
        file_outputs={
            "driver_logs": "/tmp/driver.log"  # Output logs to a file
        }
    ).after(spark_op)  # Ensure this runs after the Spark job is created
