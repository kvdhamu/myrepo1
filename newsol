{% if resources[stage]['operator'] == 'spark' %}

# Setting Spark-specific variables
spark_app_name = f"kfp-{{ stage }}-"
spark_submit_job_name = f"spark-{{ stage }}"
spark_task_params = spark_tasks_params['tasks'].get("{{ stage }}", {})

# Render Spark manifest template
spark_manifest = yaml.safe_load(spark_manifest_template.render(
    spark_app_name=spark_app_name,
    project_namespace=project_namespace,
    image="{{ image }}",
    jar_name=jar_name,
    environment_vars=env_vars,
    efs_pvc=efs_pvc,
    env_name=env_name,
    spark_main_class=spark_task_params.get('mainclass', 'Provide_mainclass_details'),
    spark_arguments=spark_task_params.get('arguments', []),
    driver_cpu_limit=str({{ resources[stage]['d_resources']['CPU_ENV_VAR_MAX'] }}),
    driver_cpu_request=str({{ resources[stage]['d_resources']['CPU_ENV_VAR_MIN'] }}),
    driver_mem_limit=(str({{ resources[stage]['d_resources']['RAM_ENV_VAR_MAX'] }})+'000M'),
    driver_mem_request=(str({{ resources[stage]['d_resources']['RAM_ENV_VAR_MIN'] }})+'000M'),
    executor_cpu_limit=str({{ resources[stage]['e_resources']['CPU_ENV_VAR_MAX'] }}),
    executor_cpu_request=str({{ resources[stage]['e_resources']['CPU_ENV_VAR_MIN'] }}),
    executor_mem_limit=(str({{ resources[stage]['e_resources']['RAM_ENV_VAR_MAX'] }})+'000M'),
    executor_mem_request=(str({{ resources[stage]['e_resources']['RAM_ENV_VAR_MIN'] }})+'000M')
))

# Submit Spark job
submit_spark_job = dsl.ResourceOp(
    name=spark_submit_job_name,
    k8s_resource=spark_manifest,
    action="create",
    attribute_outputs={
        "driver_pod_name": "{.status.driverInfo.podName}"
    },
    success_condition='status.applicationState.state == COMPLETED'
)

# Ensure proper label handling
submit_spark_job.add_pod_label(name='kubeflow/gl_pl_secret_string', value="zc095df900w459piudfkjn54sjdnf")
submit_spark_job.add_pod_label("pipelines.kubeflow.org/cache_enabled", "false")
submit_spark_job.add_pod_label("pipelines.kubeflow.org/max_cache_staleness", "P0D")

# Fetch logs from Spark driver pod
spark_logs_job_name = f"{{ stage }}-driver-logs"

fetch_driver_logs = dsl.ContainerOp(
    name=spark_logs_job_name,
    image="{{ image }}",
    command=["sh", "-c"],
    arguments=[
        f"/home/kfp-user/bin/kubectl logs {submit_spark_job.output} > /tmp/driver_pod.log && "
        "cat /tmp/driver_pod.log && " "sleep 360"
    ],
    file_outputs={
        "driver_logs": "/tmp/driver_pod.log"
    }
)

# Ensure `fetch_driver_logs` runs after Spark job
fetch_driver_logs.after(submit_spark_job)
fetch_driver_logs.add_pod_label(name='kubeflow/gl_pl_secret_string', value="zc095df900w459piudfkjn54sjdnf")
fetch_driver_logs.add_pod_label("pipelines.kubeflow.org/cache_enabled", "false")
fetch_driver_logs.add_pod_label("pipelines.kubeflow.org/max_cache_staleness", "P0D")

# Return both Spark job and logs job
return submit_spark_job, fetch_driver_logs
{% endif %}



@dsl.pipeline(
    name='{{ pipeline_name }}',
    description='{{ pipeline_description }}'
)
def pipeline(
    {% for param in input_params.keys() %}{{ param }}{% if not loop.last %}, {% endif %}{% endfor %}
):
    stages = {{ stages | tojson }}
    previous_stage_tasks = []

    for stage in stages:
        parallel_tasks = []

        for task_name in stage:
            # Dynamically create tasks
            task_result = globals()[f"{task_name.replace('-','_')}"](
                {% for param in input_params.keys() %}{{ param }}{% if not loop.last %}, {% endif %}{% endfor %}
            )

            # If Spark task, unpack both jobs (submit_spark_job, fetch_driver_logs)
            if isinstance(task_result, tuple):
                submit_spark_job, fetch_driver_logs = task_result
                parallel_tasks.append(submit_spark_job)
                parallel_tasks.append(fetch_driver_logs)
            else:
                task = task_result
                parallel_tasks.append(task)

                # Disable caching for non-Spark pod tasks
                if resources[task_name]['operator'] == 'pod':
                    task.execution_options.caching_strategy.max_cache_staleness = "P0D"
                    task.set_caching_options(False)

            # Manage dependencies correctly
            if previous_stage_tasks:
                for prev_task in previous_stage_tasks:
                    task.after(prev_task)
                    if resources[task_name]['operator'] == 'spark':
                        fetch_driver_logs.after(submit_spark_job)

        previous_stage_tasks = parallel_tasks

    dsl.get_pipeline_conf().set_ttl_seconds_after_finished(300)

compiler.Compiler().compile(pipeline, '{{pipeline_name}}_{{ run_name }}.yaml')





